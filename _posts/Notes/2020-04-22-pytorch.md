---
title: Pytorch
---

## Cuda Semantics

ref: https://pytorch.org/docs/stable/notes/cuda.html

- Cross-GPU operations are not allowed by default, with the exception of [`copy_()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.copy_) and other methods with copy-like functionality such as [`to()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.to) and [`cuda()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.cuda). Unless you enable peer-to-peer memory access, any attempts to launch ops on tensors spread across different devices will raise an error.	

## Asynchronous execution

- By default, GPU operations are asynchronous. When you call a function that uses the GPU, the operations are *enqueued* to the particular device, but not necessarily executed until later. This allows us to execute more computations in parallel, including operations on CPU or other GPUs.
  - You can force synchronous computation by setting environment variable `CUDA_LAUNCH_BLOCKING=1`. This can be handy when an error occurs on the GPU. (With asynchronous execution, such an error isn’t reported until after the operation is actually executed, so the stack trace does not show where it was requested.)
  - A consequence of the asynchronous computation is that time measurements without synchronizations are not accurate. To get precise measurements, one should either call [`torch.cuda.synchronize()`](https://pytorch.org/docs/stable/cuda.html#torch.cuda.synchronize) before measuring, or use [`torch.cuda.Event`](https://pytorch.org/docs/stable/cuda.html#torch.cuda.Event) to record times

## Loss

ref: https://pytorch.org/docs/stable/nn.html#crossentropyloss

### Cross Entropy Loss

This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class.

### Binary Cross Entropy Loss

- no softmax applied on the input

### Binary Cross Entropy Loss with Sigmoid

## View
ref: https://pytorch.org/docs/stable/tensor_view.html
PyTorch allows a tensor to be a View of an existing tensor. View tensor shares the same underlying data with its base tensor. Supporting View avoids explicit data copy, thus allows us to do fast and memory efficient reshaping, slicing and element-wise operations.

For reference, here’s a full list of view ops in PyTorch:

- Basic slicing and indexing op, e.g. `tensor[0, 2:, 1:7:2]` returns a view of base `tensor`, see note below.
- [`as_strided()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.as_strided)
- [`detach()`](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.detach)
- [`diagonal()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.diagonal)
- [`expand()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.expand)
- [`expand_as()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.expand_as)
- [`narrow()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.narrow)
- [`permute()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.permute)
- [`select()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.select)
- [`squeeze()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.squeeze)
- [`transpose()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.transpose)
- [`t()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.t)
- [`T`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.T)
- [`unfold()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.unfold)
- [`unsqueeze()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.unsqueeze)
- [`view()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view)
- [`view_as()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view_as)
- [`unbind()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.unbind)
- [`split()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.split)
- [`chunk()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.chunk)
- [`indices()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.indices) (sparse tensor only)
- [`values()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.values) (sparse tensor only)

## torch memory format

A [`torch.memory_format`](https://pytorch.org/docs/master/tensor_attributes.html#torch.torch.memory_format) is an object representing the memory format on which a [`torch.Tensor`](https://pytorch.org/docs/master/tensors.html#torch.Tensor) is or will be allocated.

Possible values are:

- `torch.contiguous_format`: Tensor is or will be allocated in dense non-overlapping memory. Strides represented by values in decreasing order.
- `torch.channels_last`: Tensor is or will be allocated in dense non-overlapping memory. Strides represented by values in `strides[0] > strides[2] > strides[3] > strides[1] == 1` aka NHWC order.
- `torch.preserve_format`: Used in functions like clone to preserve the memory format of the input tensor. If input tensor is allocated in dense non-overlapping memory, the output tensor strides will be copied from the input. Otherwise output strides will follow `torch.contiguous_format`

A `torch.layout` is an object that represents the memory layout of a `torch.Tensor`. Currently, we support `torch.strided` (dense Tensors) and have beta support for `torch.sparse_coo` (sparse COO Tensors).

## sparse

```
torch.sparse.FloatTensor(i, v, torch.Size([2,3])).to_dense()
 0  0  3
 4  0  5
[torch.FloatTensor of size 2x3]
```

- SparseTensor has the following invariants:

  sparse_dim + dense_dim = len(SparseTensor.shape)SparseTensor._indices().shape = (sparse_dim, nnz)SparseTensor._values().shape = (nnz, SparseTensor.shape[sparse_dim:])

- Since SparseTensor._indices() is always a 2D tensor, the smallest sparse_dim = 1. Therefore, representation of a SparseTensor of sparse_dim = 0 is simply a dense tensor.
- Our sparse tensor format permits *uncoalesced* sparse tensors, where there may be duplicate coordinates in the indices; in this case, the interpretation is that the value at that index is the sum of all duplicate value entries. Uncoalesced tensors permit us to implement certain operators more efficiently.
- For the most part, you shouldn’t have to care whether or not a sparse tensor is coalesced or not, as most operations will work identically given a coalesced or uncoalesced sparse tensor. However, there are two cases in which you may need to care.
  - First, if you repeatedly perform an operation that can produce duplicate entries (e.g., [`torch.sparse.FloatTensor.add()`](https://pytorch.org/docs/stable/sparse.html#torch.sparse.FloatTensor.add)), you should occasionally coalesce your sparse tensors to prevent them from growing too large.
  - Second, some operators will produce different values depending on whether or not they are coalesced or not (e.g., [`torch.sparse.FloatTensor._values()`](https://pytorch.org/docs/stable/sparse.html#torch.sparse.FloatTensor._values) and [`torch.sparse.FloatTensor._indices()`](https://pytorch.org/docs/stable/sparse.html#torch.sparse.FloatTensor._indices), as well as [`torch.Tensor.sparse_mask()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.sparse_mask)). These operators are prefixed by an underscore to indicate that they reveal internal implementation details and should be used with care, since code that works with coalesced sparse tensors may not work with uncoalesced sparse tensors; generally speaking, it is safest to explicitly coalesce before working with these operators.

## Automatic Differentiation

ref: 
- https://pytorch.org/docs/stable/notes/autograd.html
- https://pytorch.org/docs/stable/autograd.html

Keypoints:

- If there’s a single input to an operation that requires gradient, its output will also require gradient. Conversely, only if all inputs don’t require gradient, the output also won’t require it.

  - e.g. finetuning

    ```
    model = torchvision.models.resnet18(pretrained=True)
    for param in model.parameters():
        param.requires_grad = False
    # Replace the last fully-connected layer
    # Parameters of newly constructed modules have requires_grad=True by default
    model.fc = nn.Linear(512, 100)
    
    # Optimize only the classifier
    optimizer = optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9)
    ```

- Autograd is reverse automatic differentiation system. Conceptually, autograd records a graph recording all of the operations that created the data as you execute operations, giving you a directed acyclic graph whose leaves are the input tensors and roots are the output tensors. By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule.

- Internally, autograd represents this graph as a graph of `Function` objects (really expressions), which can be `apply()` ed to compute the result of evaluating the graph. 

- When computing the forwards pass, autograd simultaneously performs the requested computations and builds up a graph representing the function that computes the gradient (the `.grad_fn` attribute of each [`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor) is an entry point into this graph). 

- When the forwards pass is completed, we evaluate this graph in the backwards pass to compute the gradients.

- An important thing to note is that the graph is recreated from scratch at every iteration

  - this is exactly what allows for using arbitrary Python control flow statements, that can change the overall shape and size of the graph at every iteration. You don’t have to encode all possible paths before you launch the training - what you run is what you differentiate.

## In-place operations with autograd

Supporting in-place operations in autograd is a hard matter, and we discourage their use in most cases. Autograd’s aggressive buffer freeing and reuse makes it very efficient and there are very few occasions when in-place operations actually lower memory usage by any significant amount. Unless you’re operating under heavy memory pressure, you might never need to use them.

- There are two main reasons that limit the applicability of in-place operations:
  1. In-place operations can potentially overwrite values required to compute gradients.
  2. Every in-place operation actually requires the implementation to rewrite the computational graph. Out-of-place versions simply allocate new objects and keep references to the old graph, while in-place operations, require changing the creator of all inputs to the `Function` representing this operation. This can be tricky, especially if there are many Tensors that reference the same storage (e.g. created by indexing or transposing), and in-place functions will actually raise an error if the storage of modified inputs is referenced by any other `Tensor`.

- Inplace operations in pytorch are always postfixed with a _, like .add_() or .scatter_(). Python operations like += or *= are also inplace operations. E.g., (ref: https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#operations)

Addition: syntax 1

```
y = torch.rand(5, 3)
print(x + y)
```

Out:

```
tensor([[ 0.7623,  0.4833,  0.7200],
        [ 0.8453,  0.4220,  1.2202],
        [ 0.8108,  1.6514,  0.3858],
        [ 2.7137,  0.1767, -1.2774],
        [ 0.7659,  0.4483,  1.2968]])
```

Addition: syntax 2

```
print(torch.add(x, y))
```

Out:

```
tensor([[ 0.7623,  0.4833,  0.7200],
        [ 0.8453,  0.4220,  1.2202],
        [ 0.8108,  1.6514,  0.3858],
        [ 2.7137,  0.1767, -1.2774],
        [ 0.7659,  0.4483,  1.2968]])
```

Addition: providing an output tensor as argument

```
result = torch.empty(5, 3)
torch.add(x, y, out=result)
print(result)
```

Out:

```
tensor([[ 0.7623,  0.4833,  0.7200],
        [ 0.8453,  0.4220,  1.2202],
        [ 0.8108,  1.6514,  0.3858],
        [ 2.7137,  0.1767, -1.2774],
        [ 0.7659,  0.4483,  1.2968]])
```

Addition: in-place

```
# adds x to y
y.add_(x)
print(y)
```

Out:

```
tensor([[ 0.7623,  0.4833,  0.7200],
        [ 0.8453,  0.4220,  1.2202],
        [ 0.8108,  1.6514,  0.3858],
        [ 2.7137,  0.1767, -1.2774],
        [ 0.7659,  0.4483,  1.2968]])
```

## In-place correctness checks

Every tensor keeps a version counter, that is incremented every time it is marked dirty in any operation. When a Function saves any tensors for backward, a version counter of their containing Tensor is saved as well. Once you access `self.saved_tensors` it is checked, and if it is greater than the saved value an error is raised. This ensures that if you’re using in-place functions and not seeing any errors, you can be sure that the computed gradients are correct.

## contiguous

https://discuss.pytorch.org/t/contigious-vs-non-contigious-tensor/30107/4
Returns a contiguous in memory tensor containing the same data as self tensor

## Dataset 

ref: https://pytorch.org/docs/stable/data.html

- map-type dataset
  - `__getitem__` (and `__len__`)
  - represents a map from (possibly non-integral) indices/keys to data samples.
  - [`torch.utils.data.Sampler`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Sampler) classes are used to specify the sequence of indices/keys used in data loading. They represent iterable objects over the indices to datasets. 
  - A sequential or shuffled sampler will be automatically constructed based on the `shuffle` argument to a [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). The `sampler` is either provided by user or constructed based on the `shuffle` argument. 
  - Alternatively, users may use the `sampler` argument to specify a custom [`Sampler`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Sampler) object that at each time yields the next index/key to fetch. A custom [`Sampler`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Sampler) that yields a list of batch indices at a time can be passed as the `batch_sampler` argument. Automatic batching can also be enabled via `batch_size` and `drop_last` arguments.
    - users can alternatively specify `batch_sampler`, which yields a list of keys at a time.
- iterable-type dataset
  - `__iter__`
  - This type of datasets is particularly suitable for cases where random reads are expensive or even improbable, and where the batch size depends on the fetched data.
  - data loading order is entirely controlled by the user-defined iterable. This allows easier implementations of chunk-reading and dynamic batch size (e.g., by yielding a batched sample at each time).
  - Neither `sampler` nor `batch_sampler` is compatible with iterable-style datasets, since such datasets have no notion of a key or an index.

## Sampler

- The `batch_size` and `drop_last` arguments essentially are used to construct a `batch_sampler` from `sampler`.

- After fetching a list of samples using the indices from sampler, the function passed as the `collate_fn` argument is used to collate lists of samples into batches.

  - In this case, loading from a map-style dataset is roughly equivalent with:

    ```python
    for indices in batch_sampler:
        yield collate_fn([dataset[i] for i in indices])
    ```

  - and loading from an iterable-style dataset is roughly equivalent with:

    ```python
    dataset_iter = iter(dataset)
    for indices in batch_sampler:
        yield collate_fn([next(dataset_iter) for _ in indices])
    ```
  
  - e.g., padding sequential data to max length of a batch
  
- In certain cases, users may want to handle batching manually in dataset code, or simply load individual samples. For example, it could be cheaper to directly load batched data (e.g., bulk reads from a database or reading continuous chunks of memory), or the batch size is data dependent, or the program is designed to work on individual samples. Under these scenarios, it’s likely better to not use automatic batching (where `collate_fn` is used to collate the samples), but let the data loader directly return each member of the `dataset` object.


## matmul

Matrix product of two tensors.

The behavior depends on the dimensionality of the tensors as follows:

* If both tensors are 1-dimensional, the dot product (scalar) is returned.
```
>>> # vector x vector
>>> tensor1 = torch.randn(3)
>>> tensor2 = torch.randn(3)
>>> torch.matmul(tensor1, tensor2).size()
torch.Size([])
```

* If both arguments are 2-dimensional, the matrix-matrix product is returned.

* If the first argument is 1-dimensional and the second argument is 2-dimensional, a 1 is prepended to its dimension for the purpose of the matrix multiply. After the matrix multiply, the prepended dimension is removed.
```
# vector x matrix
>>> tensor1 = torch.randn(3)
>>> tensor2 = torch.randn(3, 4)
>>> torch.matmul(tensor1, tensor2).size()
torch.Size([4])
```

* If the first argument is 2-dimensional and the second argument is 1-dimensional, the matrix-vector product is returned.
```
>>> # matrix x vector
>>> tensor1 = torch.randn(3, 4)
>>> tensor2 = torch.randn(4)
>>> torch.matmul(tensor1, tensor2).size()
torch.Size([3])
```

* If both arguments are at least 1-dimensional and at least one argument is N-dimensional (where N > 2), then a batched matrix multiply is returned. 
```
>>> # batched matrix x batched matrix
>>> tensor1 = torch.randn(10, 3, 4)
>>> tensor2 = torch.randn(10, 4, 5)
>>> torch.matmul(tensor1, tensor2).size()
torch.Size([10, 3, 5])

>>> # batched matrix x broadcasted matrix
>>> tensor1 = torch.randn(10, 3, 4)
>>> tensor2 = torch.randn(4, 5)
>>> torch.matmul(tensor1, tensor2).size()
torch.Size([10, 3, 5])
```


  * 	If the first argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the batched matrix multiply and removed after. 
  * 	If the second argument is 1-dimensional, a 1 is appended to its dimension for the purpose of the batched matrix multiple and removed after. 
```
>>> # batched matrix x broadcasted vector
>>> tensor1 = torch.randn(10, 3, 4)
>>> tensor2 = torch.randn(4)
>>> torch.matmul(tensor1, tensor2).size()
torch.Size([10, 3])
```
  * 	The non-matrix (i.e. batch) dimensions are broadcasted (and thus must be broadcastable). For example, if input is a (j \times 1 \times n \times m)(j×1×n×m) tensor and other is a (k \times m \times p)(k×m×p) tensor, out will be an (j \times k \times n \times p)(j×k×n×p) tensor.

	Steps (From My Understanding):
	 1. checking with starting from trailing dimensions if both two are broadcastable to each other. 
	 2. Broadcast them if yes. 
	 3. Reduce the dimension to at most 3D with the dim0 be the batch dimension
	 4. Do `bmm`
	 5. Expand the dimension to the original.

```
>>> tensor1 = torch.randn(1280, 5)  # => broadcasted to (1280, 1280, 5)    [batch dimension is broadcasted from None to 1280 at dim0]
>>> tensor2 = torch.randn(1280, 5, 512)
>>> torch.matmul(tensor1, tensor2).shape  
torch.Size([1280, 1280, 512])
```
## batch matmul two 4D arrays
```
import torch as t
a=t.Tensor(10, 3, 4,5)
b=t.Tensor(10, 5,6,7)
r=a.view(10,-1,5).bmm(b.view(10,5,-1))
r = r.view(10,3,4,6,7)
```

## pad/pack sequence

`pack_sequence(data: List[Tensor]) -> PackedSequence  # data shape: B x (T x *)  # could be variable length`
`pack_padded_sequence(data: Tensor, lens: List[int]) -> PackedSequence # data shape: (B x T x *) or (T x B x *)`

```
data: 
[[1, 2, 3],
[10, 20],
[100]]
or
[[1, 2, 3],
[10, 20, 0],
[100, 0, 0]]
lens: [3, 2, 1]
=> PackedSequence(data=[1, 10, 100, 2, 20, 3], batch_sizes=[3, 2, 1])
```

`pad_sequence(data: List[Tensor]) -> Tensor`  # data shape: B x (T x *), output shape: (longest T x B x *) or (B x longest T x *)  # could be variable length

`pad_packed_sequence(data: PackedSequence) -> Tuple[Tensor, List[int]] `  # data shape: B x (T x *), output1: padded  (longest T x B x *) or (B x longest T x *), output2:   # could be variable length

```
>>> from torch.nn.utils.rnn import pad_sequence
>>> from torch.nn.utils.rnn import pack_sequence
>>> from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
```
```
>>> a = torch.ones(25, 300)
>>> b = torch.ones(22, 300)
>>> c = torch.ones(15, 300)
>>> pad_sequence([a, b, c]).size()
torch.Size([25, 3, 300])
>>> pad_sequence([a, b, c], batch_first=True).size()
torch.Size([3, 25, 300])
```

```
>>> a = torch.tensor([1,2,3])
>>> b = torch.tensor([4,5])
>>> c = torch.tensor([6])
>>> pack_sequence([a, b, c], enforce_sorted=False)  # sorted order (default)
PackedSequence(data=tensor([ 1,  4,  6,  2,  5,  3]), batch_sizes=tensor([ 3,  2,  1]), sorted_indices=None, unsorted_indices=None))

>>> pack_sequence([a, c, b])  # unsorted order
RuntimError ...
>>> pack_sequence([a, c, b], enforce_sorted=False)  # unsorted order
PackedSequence(data=tensor([1, 4, 6, 2, 5, 3]), batch_sizes=tensor([3, 2, 1]), sorted_indices=tensor([0, 2, 1]), unsorted_indices=tensor([0, 2, 1]))

```

```
>>> seq = torch.tensor([[1,2,0], [3,0,0], [4,5,6]])
>>> lens = [2, 1, 3]
>>> packed = pack_padded_sequence(seq, lens, batch_first=True, enforce_sorted=False)
>>> packed
PackedSequence(data=tensor([4, 1, 3, 5, 2, 6]), batch_sizes=tensor([3, 2, 1]),
               sorted_indices=tensor([2, 0, 1]), unsorted_indices=tensor([1, 2, 0]))
               
>>> seq_unpacked, lens_unpacked = pad_packed_sequence(packed, batch_first=True)
>>> seq_unpacked
tensor([[1, 2, 0],
        [3, 0, 0],
        [4, 5, 6]])
>>> lens_unpacked
tensor([2, 1, 3])
```

## torch.tensor

- `grad`

  This attribute is `None` by default and becomes a Tensor the first time a call to [`backward()`](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.backward) computes gradients for `self`. The attribute will then contain the gradients computed and future calls to [`backward()`](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.backward) will accumulate (add) gradients into it.

  

- `backward`(*gradient=None*, *retain_graph=None*, *create_graph=False*)

  Computes the gradient of current tensor w.r.t. graph leaves.

  The graph is differentiated using the chain rule. If the tensor is non-scalar (i.e. its data has more than one element) and requires gradient, the function additionally requires specifying `gradient`. It should be a tensor of matching type and location, that contains the gradient of the differentiated function w.r.t. `self`.

  This function accumulates gradients in the leaves - you might need to zero `.grad` attributes or set them to `None` before calling it. See [Default gradient layouts](https://pytorch.org/docs/stable/autograd.html#default-grad-layouts) for details on the memory layout of accumulated gradients.

  Parameters

  - **gradient** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor) *or* [*None*](https://docs.python.org/3/library/constants.html#None)) – Gradient w.r.t. the tensor. If it is a tensor, it will be automatically converted to a Tensor that does not require grad unless `create_graph` is True. None values can be specified for scalar Tensors or ones that don’t require grad. If a None value would be acceptable then this argument is optional.
  - **retain_graph** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – If `False`, the graph used to compute the grads will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Defaults to the value of `create_graph`.
  - **create_graph** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – If `True`, graph of the derivative will be constructed, allowing to compute higher order derivative products. Defaults to `False`.

- `is_leaf`

  The fact that gradients need to be computed for a Tensor do not mean that the [`grad`](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.grad) attribute will be populated, see [`is_leaf`](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.is_leaf) for more details.

  All Tensors that have [`requires_grad`](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.requires_grad) which is `False` will be leaf Tensors by convention.

  For Tensors that have [`requires_grad`](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.requires_grad) which is `True`, they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so `grad_fn` is None.

  Only leaf Tensors will have their [`grad`](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.grad) populated during a call to [`backward()`](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.backward). To get [`grad`](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.grad) populated for non-leaf Tensors, you can use [`retain_grad()`](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.retain_grad).

  ```
  >>> a = torch.rand(10, requires_grad=True)
  >>> a.is_leaf
  True
  >>> b = torch.rand(10, requires_grad=True).cuda()
  >>> b.is_leaf
  False
  # b was created by the operation that cast a cpu Tensor into a cuda Tensor
  >>> c = torch.rand(10, requires_grad=True) + 2
  >>> c.is_leaf
  False
  # c was created by the addition operation
  >>> d = torch.rand(10).cuda()
  >>> d.is_leaf
  True
  # d does not require gradients and so has no operation creating it (that is tracked by the autograd engine)
  >>> e = torch.rand(10).cuda().requires_grad_()
  >>> e.is_leaf
  True
  # e requires gradients and has no operations creating it
  >>> f = torch.rand(10, requires_grad=True, device="cuda")
  >>> f.is_leaf
  True
  # f requires grad, has no operation creating it
  ```

- `clone()`

Unlike copy_(), this function is recorded in the computation graph. Gradients propagating to the cloned tensor will propagate to the original tensor.

- `copy()`

- `where()`

- `t[mask, :]`

- `unsqueeze()`

- `squeeze()`

- `expand(shape)`

- `expand_as(tensor)`

- `detach()`

  Returns a new Tensor, detached from the current graph.

  The result will never require gradient.

      Returned Tensor shares the same storage with the original one. In-place modifications on either of them will be seen, and may trigger errors in correctness checks. IMPORTANT NOTE: Previously, in-place size / stride / storage changes (such as resize_ / resize_as_ / set_ / transpose_) to the returned tensor also update the original tensor. Now, these in-place changes will not update the original tensor anymore, and will instead trigger an error. For sparse tensors: In-place indices / values changes (such as zero_ / copy_ / add_) to the returned tensor will not update the original tensor anymore, and will instead trigger an error.

- 

## utils

- `torch.masked_select()`

  ```
  >>> torch.masked_select(
  	torch.tensor([[False, False, False, False],
      	[False, True, True, True],
      	[False, False, False, True]]), 
      tensor([[ 0.3552, -2.3825, -0.8297,  0.3477],
          [-1.2035,  1.2252,  0.5002,  0.6248],
          [ 0.1307, -2.0608,  0.1244,  2.0139]]))
  tensor([ 1.2252,  0.5002,  0.6248,  2.0139])
  ```

- `torch.index_select`

  ```
  >>> t = torch.tensor([[1,2,3],[4,5,6],[7,8,9]])
  >>> torch.index_select`(t, 0, torch.tensor([0, 2]]))
  tensor([[1,2,3],[7,8,9]])
  >>> torch.index_select`(t, 1, torch.tensor([0, 2]]))
  tensor([[1,3],[4,6],[7,9]])
  ```

- `torch.gather()`

  ```
  >>> t = torch.tensor([[1,2],[3,4]])
  >>> torch.gather(t, 1, torch.tensor([[0,0],[1,0]]))
  tensor([[ 1,  1],
          [ 4,  3]])
  ```

- `torch.where(condition/mask: Tensor, x: Tensor, y: Tensor)`

- `torch.randint(low=0, high=2, size=(4, 5))`: uniform

- `torch.rand()`: uniform

- `torch.randn(4, 5)`: normal

- `torch.normal(mean=[1, 2, 3], std=[0.1, 0.05, 0.01])`

- `torch.randn/randint_like`

- `torch.multinomial(input, num_samples, replacement=False, *, generator=None, out=None) → LongTensor`

  input: probabilities of each slot/index drawn

  ```
  >>> weights = torch.tensor([13.7, 10, 3, 0], dtype=torch.float)
  >>> torch.multinomial(weights, 2)
  tensor([0, 1])
  ```

- `torch.expand()`

- `torch.chunk(input, chunks, dim=0)`

- `torch.split(tensor, split_size_or_sections: Union[int, List[int]])`

## torch.nn.Module

- `apply()`

`module.apply(init_weights)`  # recursively into submodules

## bfloat16 floating-point format

The **bfloat16 (Brain Floating Point) floating-point format** is a [computer number format](https://en.wikipedia.org/wiki/Computer_number_format) occupying [16 bits](https://en.wikipedia.org/wiki/16-bit) in [computer memory](https://en.wikipedia.org/wiki/Computer_memory); it represents a wide [dynamic range](https://en.wikipedia.org/wiki/Dynamic_range) of numeric values by using a [floating radix point](https://en.wikipedia.org/wiki/Floating_point). This format is a truncated (16-bit) version of the 32-bit [IEEE 754 single-precision floating-point format](https://en.wikipedia.org/wiki/Single-precision_floating-point_format) (binary32) with the intent of [accelerating](https://en.wikipedia.org/wiki/Hardware_acceleration) [machine learning](https://en.wikipedia.org/wiki/Machine_learning) and [near-sensor computing](https://en.wikipedia.org/wiki/Intelligent_sensor).[[1\]](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format#cite_note-1) It preserves the approximate dynamic range of 32-bit floating-point numbers by retaining 8 [exponent bits](https://en.wikipedia.org/wiki/Exponent_bias), but supports only an 8-bit precision rather than the 24-bit [significand](https://en.wikipedia.org/wiki/Significand) of the binary32 format. More so than single-precision 32-bit floating-point numbers, bfloat16 numbers are unsuitable for integer calculations, but this is not their intended use. Bfloat16 is used to reduce the storage requirements and increase the calculation speed of machine learning algorithms.[[2\]](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format#cite_note-Why-2)

## `del` and `torch.empty_cache()`

Pytorch will allocate memory if needed (dynamically). If a tensor is `del`ed, the memory allocated will not be released, but those area in the GPU memory can be overriden.

If you really want to free the pre-allocated memory, use `torch.empty_cache()`.

## `torch.tensor()`/`requires_grad_()`/`detach()`

[`torch.tensor()`](https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor) always copies `data`. If you have a Tensor `data` and just want to change its `requires_grad` flag, use [`requires_grad_()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.requires_grad_) or [`detach()`](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.detach) to avoid a copy. If you have a numpy array and want to avoid a copy, use [`torch.as_tensor()`](https://pytorch.org/docs/stable/generated/torch.as_tensor.html#torch.as_tensor).

### detach()

- Returns a new Tensor, detached from the current graph.
- Returned Tensor **shares the same storage** with the original one. 
- **In-place modifications on either of them will be seen**, and may trigger errors in correctness checks. 
  - IMPORTANT NOTE (Exceptions - resize_ / resize_as_ / set_ / transpose_ for dense & zero_ / copy_ / add_ for sparse): 
    - Previously, in-place size / stride / storage changes (such as resize_ / resize_as_ / set_ / transpose_) to the returned tensor also update the original tensor. 
    - Now, these in-place changes will not update the original tensor anymore, and will instead trigger an error. 
    - For sparse tensors: In-place indices / values changes (such as zero_ / copy_ / add_) to the returned tensor will not update the original tensor anymore, and will instead trigger an error.

### `detach_`()

Detaches the Tensor from the graph that created it, making it a leaf. Views cannot be detached in-place.

## `item()`

Use [`torch.Tensor.item()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.item) to get a Python number from a tensor containing a single value

`some_tensor.data`

The `.data` field is an old field that is kept for backward compatibility but should not be used anymore as it’s usage is dangerous and can make computations wrong. You should use `.detach()` and/or `with torch.no_grad()` instead now.

ref: https://discuss.pytorch.org/t/the-difference-between-torch-tensor-data-and-torch-tensor/25995/2

## Optimizer & Scheduler

ref: https://pytorch.org/docs/stable/optim.html#how-to-use-an-optimizer

- Learning rate scheduling should be applied after optimizer’s update
- Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before the optimizer’s update; 1.1.0 changed this behavior in a BC-breaking way. If you use the learning rate scheduler (calling `scheduler.step()`) before the optimizer’s update (calling `optimizer.step()`), this will skip the first value of the learning rate schedule. If you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check if you are calling `scheduler.step()` at the wrong time.