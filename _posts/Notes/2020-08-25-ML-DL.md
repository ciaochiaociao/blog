---
title: Machine Learning and Deep Learning
---

## Label Smoothing

ref: https://towardsdatascience.com/what-is-label-smoothing-108debd7ef06

- used by *Transformer* in *All you need is attention!*
- it is also a kind of regularization
- only applicable in cross entropy loss with softmax
- useful in classification task
- it models the noise
- it tackles not only the problem of **overfitting** but also **overconfidence**
- it is also falling under the field of **Neural Networks Calibration** (like other techniques such as *temperature scaling*)

```
y_ls = (1 - α) * y_hot + α / K
```



![Image for post](https://miro.medium.com/max/1050/1*wnaItlASKNzBk4m4M_CHng.png)

```
# https://github.com/pytorch/pytorch/issues/7455

class LabelSmoothingLoss(nn.Module):
    def __init__(self, classes, smoothing=0.0, dim=-1):
        super(LabelSmoothingLoss, self).__init__()
        self.confidence = 1.0 - smoothing
        self.smoothing = smoothing
        self.cls = classes
        self.dim = dim

    def forward(self, pred, target):
        pred = pred.log_softmax(dim=self.dim)
        with torch.no_grad():
            # true_dist = pred.data.clone()
            true_dist = torch.zeros_like(pred)
            true_dist.fill_(self.smoothing / (self.cls - 1))
            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)
        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))
        

# https://medium.com/towards-artificial-intelligence/how-to-use-label-smoothing-for-regularization-aa349f7f1dbb

def linear_combination(x, y, epsilon): 
    return epsilon*x + (1-epsilon)*y
    
    
def reduce_loss(loss, reduction='mean'):
    return loss.mean() if reduction=='mean' else loss.sum() if reduction=='sum' else loss


class LabelSmoothingCrossEntropy(nn.Module):
    def __init__(self, epsilon:float=0.1, reduction='mean'):
        super().__init__()
        self.epsilon = epsilon
        self.reduction = reduction
    
    def forward(self, preds, target):
        n = preds.size()[-1]
        log_preds = F.log_softmax(preds, dim=-1)
        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)
        nll = F.nll_loss(log_preds, target, reduction=self.reduction)
        return linear_combination(loss/n, nll, self.epsilon)
        

# https://github.com/OpenNMT/OpenNMT-py/blob/e8622eb5c6117269bb3accd8eb6f66282b5e67d9/onmt/utils/loss.py#L186

class LabelSmoothingLoss(nn.Module):
    """
    With label smoothing,
    KL-divergence between q_{smoothed ground truth prob.}(w)
    and p_{prob. computed by model}(w) is minimized.
    """
    def __init__(self, label_smoothing, tgt_vocab_size, ignore_index=-100):
        assert 0.0 < label_smoothing <= 1.0
        self.ignore_index = ignore_index
        super(LabelSmoothingLoss, self).__init__()

        smoothing_value = label_smoothing / (tgt_vocab_size - 2)
        one_hot = torch.full((tgt_vocab_size,), smoothing_value)
        one_hot[self.ignore_index] = 0
        self.register_buffer('one_hot', one_hot.unsqueeze(0))

        self.confidence = 1.0 - label_smoothing

    def forward(self, output, target):
        """
        output (FloatTensor): batch_size x n_classes
        target (LongTensor): batch_size
        """
        model_prob = self.one_hot.repeat(target.size(0), 1)
        model_prob.scatter_(1, target.unsqueeze(1), self.confidence)
        model_prob.masked_fill_((target == self.ignore_index).unsqueeze(1), 0)

        return F.kl_div(output, model_prob, reduction='sum')
```



references:

- https://towardsdatascience.com/what-is-label-smoothing-108debd7ef06
- I. Goodfellow, Y. Bengio, and A. Courville. [Deep Learning](http://www.deeplearningbook.org/) (2016), MIT Press. Chapter 7.5.1 Regularization - Noise Robustness - Injecting Noise at the Output Targets



## Batch/Layer/Instance/Group/Weight Normalization

ref: 

- https://mlexplained.com/2018/11/30/an-overview-of-normalization-methods-in-deep-learning/
- https://mlexplained.com/2018/01/13/weight-normalization-and-layer-normalization-explained-normalization-in-deep-learning-part-2/
- http://mlexplained.com/2018/01/10/an-intuitive-explanation-of-why-batch-normalization-really-works-normalization-in-deep-learning-part-1/

![img](https://i1.wp.com/mlexplained.com/wp-content/uploads/2018/11/Screen-Shot-2018-11-28-at-4.56.06-PM.png?fit=522%2C153&ssl=1)

### Weight Normalization

![\boldsymbol{w} = \frac{g}{\| \boldsymbol{v} \|}\boldsymbol{v}](https://s0.wp.com/latex.php?latex=%5Cboldsymbol%7Bw%7D+%3D+%5Cfrac%7Bg%7D%7B%5C%7C+%5Cboldsymbol%7Bv%7D+%5C%7C%7D%5Cboldsymbol%7Bv%7D&bg=ffffff&fg=000&s=0)

- ref: https://arxiv.org/pdf/1602.07868.pdf
- In [weight normalization](https://arxiv.org/pdf/1602.07868.pdf), instead of normalizing the activations directly, we **normalize the weights of the layer**.
- Similar to batch normalization, weight normalization **separates the norm of the weight vector from its direction without reducing expressiveness**. This has a similar effect to dividing the inputs by the standard deviation in batch normalization.
- As for the mean, the authors of the paper proposed using a method called "mean-only batch normalization" together with weight normalization. Basically, they subtract out the mean of the minibatch but do not divide by the standard deviation. Compared to the standard deviation, the mean apparently has a "gentler" noise thanks to the law of large numbers, so weight normalization can still work in settings with a smaller minibatch size.

### Batch Normalization

- http://mlexplained.com/2018/01/10/an-intuitive-explanation-of-why-batch-normalization-really-works-normalization-in-deep-learning-part-1/
- When we introduce batch normalization, the loss value for each sample in a minibatch becomes dependent **on other samples in the minibatch**
- Also, **batch normalization makes the loss function dependent on the batch size**  (a smaller minibatch size will increase the random variation in the mean and variance statistics)
  - we need to take extra care in choosing the batch size and learning rate in the presence of batch normalization when doing distributed training. If two different machines use different batch sizes, they will indirectly be optimizing **different loss functions**: this means that the value of gamma that worked for one machine is unlikely to work for another machine. This is why the authors stressed that the batch size for each worker must be kept constant across all machines.
- Suppose we fine-tune a ResNet50 by freezing all the layers except the last layer. An interesting question is whether we should use the mean and variance computed on the **original dataset** or use the mean and variance of the mini-batches.
  -  Though most frameworks use the mini-batch statistics, if we are using a different mini-batch size there will be a mismatch between the optimal batch normalization parameters and the parameters in the network. As [this thread discusses](https://forums.fast.ai/t/freezing-batch-norm/8377/5), it might be better to use the statistics of the original dataset instead.

![\mu_j = \frac{1}{m}\sum_{i=1}^{m}x_{ij} \\  \sigma_j^2 = \frac{1}{m}\sum_{i=1}^{m}(x_{ij} - \mu_j)^2 \\  \hat{x_{ij}} = \frac{x_{ij} - \mu_j}{\sqrt{\sigma_j^2 + \epsilon}}  ](https://s0.wp.com/latex.php?latex=%5Cmu_j+%3D+%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5E%7Bm%7Dx_%7Bij%7D+%5C%5C++%5Csigma_j%5E2+%3D+%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%28x_%7Bij%7D+-+%5Cmu_j%29%5E2+%5C%5C++%5Chat%7Bx_%7Bij%7D%7D+%3D+%5Cfrac%7Bx_%7Bij%7D+-+%5Cmu_j%7D%7B%5Csqrt%7B%5Csigma_j%5E2+%2B+%5Cepsilon%7D%7D++&bg=ffffff&fg=000&s=0)

### Layer Normalization

- ref: https://arxiv.org/pdf/1607.06450.pdf
- it **normalizes the inputs across the features**.
-  layer normalization is not a simple reparameterization of the network, unlike the case of weight normalization and batch normalization, which both have the same expressive power as an unnormalized neural network.
- **performs well on RNNs**
- used in Vaswani's Transformer a lot

![\mu_i = \frac{1}{m}\sum_{j=1}^{m}x_{ij} \\  \sigma_i^2 = \frac{1}{m}\sum_{j=1}^{m}(x_{ij} - \mu_i)^2 \\  \hat{x_{ij}} = \frac{x_{ij} - \mu_i}{\sqrt{\sigma_i^2 + \epsilon}}  ](https://s0.wp.com/latex.php?latex=%5Cmu_i+%3D+%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bj%3D1%7D%5E%7Bm%7Dx_%7Bij%7D+%5C%5C++%5Csigma_i%5E2+%3D+%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bj%3D1%7D%5E%7Bm%7D%28x_%7Bij%7D+-+%5Cmu_i%29%5E2+%5C%5C++%5Chat%7Bx_%7Bij%7D%7D+%3D+%5Cfrac%7Bx_%7Bij%7D+-+%5Cmu_i%7D%7B%5Csqrt%7B%5Csigma_i%5E2+%2B+%5Cepsilon%7D%7D++&bg=ffffff&fg=000&s=0)

where ![x_{ij} ](https://s0.wp.com/latex.php?latex=x_%7Bij%7D+&bg=ffffff&fg=000&s=0) is the i,j-th element of the input, the first dimension represents the batch and the second represents the feature

![img](https://i1.wp.com/mlexplained.com/wp-content/uploads/2018/01/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88-2018-01-11-11.48.12.png?resize=628%2C366)

### Instance Normalization

- ref: https://arxiv.org/pdf/1607.08022.pdf
-  is similar to layer normalization but goes one step further: it computes the mean/standard deviation and normalize across **each channel in each training example**.
- Originally devised for style transfer, the problem instance normalization tries to address is that the network should be agnostic to the **contrast** of the original image
- specific to images and not trivially extendable to RNNs.
- Recently, instance normalization has also been used as a replacement for batch normalization in GANs

### Group Normalization

- ref: https://arxiv.org/pdf/1803.08494.pdf
- computes the mean and standard deviation over **groups of channels** for each training example.
- when we put all the channels into a single group, group normalization becomes layer normalization and when we put each channel into a different group it becomes instance normalization.
- Though layer normalization and instance normalization were both effective on RNNs and style transfer respectively, they were still inferior to batch normalization for image recognition tasks.
-  Group normalization was able to achieve much closer performance to batch normalization with a batch size of 32 on ImageNet and outperformed it on **smaller batch sizes**.
-  For tasks like object detection and segmentation that use much higher resolution images (and therefore cannot increase their batch size due to memory constraints), group normalization was shown to be a very effective normalization method
- One of the implicit assumptions that layer normalization makes is that all channels are "equally important" when computing the mean. This assumption is not always true in convolution layers. 
- Channels in an image are not completely independent though, so being able to leverage the statistics of nearby channels is an advantage group normalization has over instance normalization.

### Batch Renormalization

- ref: https://arxiv.org/pdf/1702.03275.pdf

- we do not use the individual mini-batch statistics for batch normalization during inference. Instead, we use a **moving average** of the mini batch statistics.

- a moving average provides a better estimate of the true mean and variance compared to individual mini-batches.

  - why don't we use the moving average during training? 

    > ​	 The answer has to do with the fact that during training, we need to perform backpropagation. In essence, when we use some statistics to normalize the data, we need to **backpropagate through those statistics as well**. If we use the statistics of activations from previous mini-batches to normalize the data, we need to account for how the previous layer affected those statistics during backpropagation. If we ignore these interactions, we could potentially cause previous layers to keep on increasing the magnitude of their activations even though it has no effect on the loss. This means that if we use a moving average, we would need to store the data from **all previous mini-batches** during training, which is far too expensive.

- a simple reparameterization of normalization with the moving average
- The trick here is to **not backpropagate** through gamma and d. Though this means we ignore some of the effects of previous layers on previous mini batches, since the mini batch statistics and moving average statistics should be the same on average, the overall effect of this should cancel out on average as well.
- still degrades when the batch size decreases (though not as badly as batch normalization)

![img](https://i1.wp.com/mlexplained.com/wp-content/uploads/2018/11/Screen-Shot-2018-11-29-at-2.18.09-PM.png?resize=427%2C74)

### Batch-Instance Normalization

- ref: https://arxiv.org/pdf/1805.07925.pdf
-  the balancing parameter ![\rho ](https://s0.wp.com/latex.php?latex=%5Crho+&bg=ffffff&fg=000&s=0) is **learned** through gradient descent
- In image classification tasks, the value of ![\rho ](https://s0.wp.com/latex.php?latex=%5Crho+&bg=ffffff&fg=000&s=0) tended to be close to 0 or 1, meaning many layers used either instance or batch normalization almost exclusively. In addition, layers tended to use batch normalization more than instance normalization, which fits the intuition proposed by the authors that instance normalization serves more as a method to eliminate unnecessary style variation.
-  On style transfer - on the other hand - the model tended to use instance normalization more, which makes sense given style is much less important in style transfer.

![img](https://i2.wp.com/mlexplained.com/wp-content/uploads/2018/11/Screen-Shot-2018-11-29-at-3.48.00-PM.png?resize=334%2C49)

### Switchable Normalization

- ref: https://arxiv.org/pdf/1811.07727v1.pdf
- switchable normalization, a method that uses a weighted average of different mean and variance statistics from batch normalization, instance normalization, and layer normalization.
-  the weights were learned through backpropagation.
- the statistics of instance normalization were used more heavily in earlier layers, whereas layer normalization was preferred in the later layers, and batch normalization being used in the middle
-  Smaller batch sizes lead to a preference towards layer normalization and instance normalization

### Spectral Normalization

- ref: https://arxiv.org/pdf/1805.07925.pdf

- Spectral normalization was a method proposed to improve the training of GANs by limiting the Lipschitz constant of the discriminator.

  - The Lipschitz constant - in case you are not familiar - is a constan L for a function f where for any x and y, 

    ![|| f(x) - f(y) || \leq L || x - y || ](https://s0.wp.com/latex.php?latex=%7C%7C+f%28x%29+-+f%28y%29+%7C%7C+%5Cleq+L+%7C%7C+x+-+y+%7C%7C+&bg=ffffff&fg=000&s=0)

- The authors restrict the Lipschitz constant by normalizing the weight matrices by their largest eigenvalue (or their spectral norm - hence the name). The largest eigenvalue is computed using the [power method](https://en.wikipedia.org/wiki/Power_iteration) which makes the computational cost of this method very cheap.

- Compared to weight normalization, spectral normalization does not reduce the rank of the weight matrix.

### ScaleNorm

- ref: https://arxiv.org/pdf/1910.05895.pdf
- a new form of normalization for the Transformer model
- centering the activations does not really matter, and that the scale is what is important.
- Instead of learning a desirable mean and variance, they cut the number of learnable parameters in half and learn a scale to which they adust the magnitude of the activations.
- The authors combine ScaleNorm with Weight Normalization and found the results roughly matched those of Layer Normalization on machine translation.

![\text{ScaleNorm}(x; g) = g \frac{x}{||x||} ](https://s0.wp.com/latex.php?latex=%5Ctext%7BScaleNorm%7D%28x%3B+g%29+%3D+g+%5Cfrac%7Bx%7D%7B%7C%7Cx%7C%7C%7D+&bg=ffffff&fg=000&s=0)